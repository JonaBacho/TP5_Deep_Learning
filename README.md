# üß† TP5: Sequence Modeling & Attention Mechanisms - MLOps Pipeline

[![TP5 Training](https://github.com/JonaBacho/TP5_Deep_Learning/actions/workflows/tp5-sequence-training.yml/badge.svg)](https://github.com/JonaBacho/TP5_Deep_Learning/actions/workflows/tp5-sequence-training.yml)
[![MLflow](https://img.shields.io/badge/MLflow-2.9.2-blue.svg)](https://mlflow.org/)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.15-orange.svg)](https://tensorflow.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

**√âcole Nationale Sup√©rieure Polytechnique de Yaound√©**  
D√©partement de G√©nie Informatique - 5GI  
Instructeurs: Louis Fippo

Ce projet impl√©mente le **TP5 sur les m√©canismes d'attention et le traitement de s√©quences**. Il explore l'√©volution des RNNs vers les architectures avec attention, incluant un d√©fi de recherche sur l'am√©lioration d'un mod√®le latent temporel (TAP - ArXiv:2102.05095) pour mieux g√©rer les d√©pendances long-terme.

---

## üéØ Objectifs d'Apprentissage

- **Impl√©menter** et visualiser le Scaled Dot-Product Attention
- **Hybrider** RNNs (LSTM/GRU) avec des couches d'Attention pour seq2seq
- **Comprendre** l'architecture des Temporal Latent Space Models
- **Rechercher** et proposer des am√©liorations architecturales pour d√©pendances long-terme
- **Pratiquer** l'√©criture scientifique pour conf√©rences AI (format NeurIPS/ICLR)

---

## üìÅ Structure du Projet

```text
.
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ mlflow_config.py              # Configuration MLflow centralis√©e
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ attention_mechanism.py        # Exercise 1: Attention de base
‚îÇ   ‚îú‚îÄ‚îÄ lstm_attention_seq2seq.py     # Exercise 2: Seq2Seq + Attention
‚îÇ   ‚îú‚îÄ‚îÄ tap_improvement.py            # Exercise 3: Improved TAP
‚îÇ   ‚îú‚îÄ‚îÄ app.py                        # API Flask (optionnel)
‚îÇ   ‚îú‚îÄ‚îÄ auto_promote.py               # Promotion automatique
‚îÇ   ‚îî‚îÄ‚îÄ promote_model.py              # Gestion manuelle des stages
‚îú‚îÄ‚îÄ paper/                            # Article scientifique
‚îÇ   ‚îú‚îÄ‚îÄ main.tex                      # Paper LaTeX
‚îÇ   ‚îú‚îÄ‚îÄ figures/                      # Diagrammes architectures
‚îÇ   ‚îî‚îÄ‚îÄ references.bib                # Bibliographie
‚îú‚îÄ‚îÄ attention_results/                # Visualisations attention
‚îú‚îÄ‚îÄ seq2seq_results/                  # R√©sultats Seq2Seq
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ test_model.py                 # Tests unitaires
‚îú‚îÄ‚îÄ .github/workflows/
‚îÇ   ‚îú‚îÄ‚îÄ tp5-sequence-training.yml     # Pipeline principal
‚îÇ   ‚îî‚îÄ‚îÄ deploy.yml                    # D√©ploiement API (optionnel)
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ Dockerfile
‚îî‚îÄ‚îÄ README.md
```

---

## üöÄ Exercices du TP5

### Part 1: Mastering Basic Attention (2h)

#### Th√©orie: Why Attention?

**Questions th√©oriques**:
1. **Scaled Dot-Product Attention**: Formule math√©matique avec Q, K, V
   ```
   Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) V
   ```
   - Pourquoi le scaling factor `1/sqrt(d_k)` est n√©cessaire?
   - R√©ponse: √âvite les valeurs extr√™mes dans softmax quand d_k est grand

2. **Self-Attention vs Cross-Attention**:
   - Self-Attention: Q, K, V de la M√äME s√©quence
   - Cross-Attention: Q d'une s√©quence, K, V d'une AUTRE

#### Exercise 1: Basic Attention Layer

Impl√©mentation d'une couche d'attention custom dans Keras.

**Architecture**:
```
Input ‚Üí GRU(return_sequences=True) ‚Üí SimpleAttention ‚Üí Dense ‚Üí Output
```

**Exp√©rience MLflow**: `TP5-Exercise1-BasicAttention`

**M√©triques**:
- `test_accuracy`: Classification accuracy
- `attention_span`: Nombre de time steps avec poids significatifs (>0.05)
- Visualisations des poids d'attention

**Dataset**: S√©quences synth√©tiques (3 classes) avec patterns temporels

### Part 2: Seq2Seq with Memory (3h)

#### Exercise 2: LSTM-Attention for Time Series

Mod√®le hybride pour pr√©diction de s√©ries temporelles.

**Architecture**:
```
Encoder: Bidirectional LSTM
Decoder: LSTM + Bahdanau (Additive) Attention
```

**Flux**:
1. Encoder encode la s√©quence d'entr√©e
2. Decoder g√©n√®re la s√©quence de sortie
3. √Ä chaque step, Attention se focalise sur parties pertinentes de l'input

**Exp√©rience MLflow**: `TP5-Exercise2-LSTM-Attention-Seq2Seq`

**M√©triques**:
- `test_loss` (MSE)
- `test_mae`
- `attention_span`: Port√©e de l'attention
- `avg_attention_position`: Position moyenne focalis√©e

**Dataset**: S√©ries temporelles synth√©tiques (combinaison sinus + trends)
- Input: 50 time steps
- Output: 10 time steps (pr√©diction)

### Part 3: Research Challenge - TAP Improvement (6h+)

#### Context: ArXiv 2102.05095

**Paper**: "Temporal Latent Space Modeling for Video Generation" (TAP)

**Probl√®me**: TAP peut √™tre am√©lior√© pour maintenir la coh√©rence sur fen√™tres temporelles tr√®s longues

#### Challenge: Long-Term Consistency

Proposer une modification architecturale pour mieux g√©rer les d√©pendances long-terme.

**Am√©liorations Impl√©ment√©es**:

1. **Temporal Transformer Block**
   - Multi-head attention pour d√©pendances long-range
   - Remplace transitions temporelles standard

2. **Memory-Augmented Module**
   - Module de m√©moire externe (32 slots)
   - Stocke "keyframes" importants
   - Inspir√© des Memory Networks

3. **Hierarchical Temporal Encoder**
   - 3 niveaux temporels:
     - Niveau 1: Court-terme (frames individuels)
     - Niveau 2: Moyen-terme (segments)
     - Niveau 3: Long-terme (s√©quence compl√®te)
   - Pooling multi-√©chelle

**Architecture Improved TAP**:
```
Input Sequence
    ‚Üì
Hierarchical Temporal Encoder (3 levels)
    ‚Üì
Temporal Transformer (Multi-head Attention)
    ‚Üì
Memory Module (Keyframe Storage)
    ‚Üì
Latent Space Projection
    ‚Üì
Decoder (LSTM + Dense)
    ‚Üì
Reconstruction
```

**Exp√©rience MLflow**: `TP5-Exercise3-ImprovedTAP`

**M√©triques**:
- `test_loss` (MSE reconstruction)
- `test_mae`
- Analyse qualitative de la coh√©rence temporelle

**Dataset**: Moving MNIST-like (simplifi√©)
- Objets en mouvement sur 16 frames
- Test des d√©pendances long-terme

#### Submission: Scientific Paper

**Format**: 4 pages (+ r√©f√©rences) style NeurIPS/ICLR

**Sections requises**:
1. **Abstract**: Probl√®me et am√©lioration propos√©e
2. **Introduction**: Motivation pour d√©pendances long-terme
3. **Proposed Method**: Description math√©matique et architecturale
4. **Experiments**: Comparaison TAP original vs Improved
5. **Ablation Study**: Impact de chaque module
6. **Conclusion**: Limitations et travaux futurs

**Contrainte**: Pas de transformers pr√©-entra√Æn√©s "black-box"

---

## üõ†Ô∏è Installation

### Pr√©requis
- Python 3.10+
- TensorFlow 2.15
- LaTeX (pour compiler le paper)
- Acc√®s √† un serveur MLflow

### Installation Locale

```bash
git clone <votre-repo-url>
cd TP5_Sequence_Attention

python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

pip install -r requirements.txt

cp .env.example .env
# √âditer .env avec credentials MLflow
```

---

## üîå Utilisation

### Ex√©cution via GitHub Actions

```bash
# Push ‚Üí d√©clenche automatiquement
git push origin main

# Ou manuel depuis GitHub UI
# Actions ‚Üí "TP5 - Sequence Modeling & Attention Training"
# Choisir: all, exercise1, exercise2, exercise3
```

**Dur√©e**:
- Exercise 1: ~15 min
- Exercise 2: ~25 min
- Exercise 3: ~35 min
- Total: ~35-40 min (parall√®le)

### Ex√©cution Locale

```bash
# Exercise 1: Basic Attention
python src/attention_mechanism.py

# Exercise 2: Seq2Seq Attention
python src/lstm_attention_seq2seq.py

# Exercise 3: Improved TAP
python src/tap_improvement.py
```

---

## ü§ñ Pipeline CI/CD

### 1. TP5 - Sequence Training (`tp5-sequence-training.yml`)

**Jobs**:
1. **exercise1-basic-attention** (15 min)
   - GRU + Attention
   - Visualisations poids d'attention
   
2. **exercise2-lstm-attention** (25 min)
   - Seq2Seq Bi-LSTM + Bahdanau Attention
   - Heatmap attention temporelle

3. **exercise3-tap-improvement** (35 min)
   - Improved TAP architecture
   - Transformer + Memory + Hierarchical

4. **promote-best-model**
   - Compare les 3 exercices
   - Promeut le meilleur mod√®le

5. **summary**
   - Rapport consolid√©
   - Artifacts: logs + visualisations

---

## üìä Visualisation des R√©sultats

### Dans MLflow UI

```bash
# Exp√©riences cr√©√©es:
- TP5-Exercise1-BasicAttention             (1 run)
- TP5-Exercise2-LSTM-Attention-Seq2Seq     (1 run)
- TP5-Exercise3-ImprovedTAP                (1 run)
- TP5-Theory-ScaledAttention               (1 run - analyse th√©orique)
```

**M√©triques Exercise 1**:
- `test_accuracy`
- `average_attention_span`

**M√©triques Exercise 2**:
- `test_loss`, `test_mae`
- `attention_span`
- `avg_attention_position`

**M√©triques Exercise 3**:
- `test_loss`, `test_mae`
- `trainable_parameters`

**Visualisations**:
- Poids d'attention (heatmaps)
- Attention weights par time step
- Reconstruction vid√©o (Moving MNIST)

---

## ‚öôÔ∏è Configuration

### Variables d'Environnement

```bash
MLFLOW_TRACKING_URI=http://your-mlflow-server:5000
MLFLOW_TRACKING_USERNAME=your_username
MLFLOW_TRACKING_PASSWORD=your_password

MODEL_NAME=sequence-attention-model

MIN_ACCURACY=0.75  # Pour promotion

MLFLOW_S3_ENDPOINT_URL=https://your-s3
AWS_ACCESS_KEY_ID=your_key
AWS_SECRET_ACCESS_KEY=your_secret
```

---

## üìà R√©sultats Attendus

### Exercise 1: Basic Attention
```
Dataset: 1000 synthetic sequences (50 time steps)
Architecture: GRU + Attention
Test Accuracy: ~0.85-0.90
Attention Span: ~15-20 time steps
Training Time: ~10 min (20 epochs)
```

### Exercise 2: Seq2Seq Attention
```
Task: Time series forecasting (50‚Üí10 steps)
Architecture: Bi-LSTM Encoder + LSTM Decoder + Bahdanau Attention
Test MAE: ~0.15-0.20
Attention Span: ~25-30 time steps
Training Time: ~20 min (30 epochs)
Observation: Attention se focalise sur patterns pertinents
```

### Exercise 3: Improved TAP
```
Task: Video reconstruction (16 frames)
Improvements: Transformer + Memory + Hierarchical
Test MSE: ~0.01-0.02
Test MAE: ~0.08-0.12
Parameters: ~1-2M
Training Time: ~30 min (50 epochs)

Benefits:
- Better long-term consistency
- Reduced error accumulation
- Keyframe memory helps periodic motions
```

---

## üìù Scientific Paper

### Structure Template (LaTeX)

```latex
\documentclass{article}
\usepackage{neurips_2024}
\usepackage{tikz, amsmath, graphicx}

\title{Improving Temporal Latent Space Models with \\
       Memory-Augmented Hierarchical Attention}

\author{Your Name \\ ENSPY, Universit√© de Yaound√© I}

\begin{document}

\maketitle

\begin{abstract}
Long-term temporal consistency remains a challenge...
\end{abstract}

\section{Introduction}
...

\section{Related Work}
\subsection{Temporal Latent Space Models}
\subsection{Attention Mechanisms}
\subsection{Memory Networks}

\section{Proposed Method}
\subsection{Hierarchical Temporal Encoding}
\subsection{Temporal Transformer}
\subsection{Memory-Augmented Module}

\section{Experiments}
\subsection{Experimental Setup}
\subsection{Quantitative Results}
\subsection{Qualitative Analysis}
\subsection{Ablation Study}

\section{Conclusion}

\bibliographystyle{plain}
\bibliography{references}

\end{document}
```

### Figures √† Inclure

1. Architecture diagram (TikZ)
2. Attention weights heatmaps
3. Reconstruction quality comparisons
4. Ablation study results (bar charts)

---

## üß™ Tests

```bash
pytest tests/test_model.py

# Test Attention Layer
python -c "
from src.attention_mechanism import SimpleAttention
import tensorflow as tf
layer = SimpleAttention()
x = tf.random.normal((2, 10, 64))
context, weights = layer(x)
print(f'Context: {context.shape}')
print(f'Weights: {weights.shape}')
"

# Test Seq2Seq
python -c "
from src.lstm_attention_seq2seq import build_seq2seq_attention_model
model, _, _ = build_seq2seq_attention_model(50, 10)
model.summary()
"
```

---

## üêõ Troubleshooting

### Out of Memory (Seq2Seq)

```python
# R√©duire batch size et s√©quences
model.fit(..., batch_size=8)  # au lieu de 32
```

### TAP Training Slow

```bash
# R√©duire epochs ou latent_dim
python src/tap_improvement.py
# Modifier epochs=30 au lieu de 50
# Modifier latent_dim=64 au lieu de 128
```

### LaTeX Paper Compilation

```bash
cd paper/
pdflatex main.tex
bibtex main
pdflatex main.tex
pdflatex main.tex
```

---

## üìö Ressources

- [√ânonc√© TP5 (PDF)](./TP5_DL_5GI_2025_EN.pdf)
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [Neural Machine Translation (Bahdanau)](https://arxiv.org/abs/1409.0473)
- [TAP Paper (ArXiv:2102.05095)](https://arxiv.org/abs/2102.05095)
- [Memory Networks](https://arxiv.org/abs/1410.3916)
- [NeurIPS LaTeX Template](https://neurips.cc/Conferences/2024/PaperInformation/StyleFiles)

---

## üë• Auteurs

**ENSPY - Universit√© de Yaound√© I**  
FOMEKONG TAMDJI JONATHAN BACHELARD
D√©partement de G√©nie Informatique - Promotion 5GI 2025

**Instructeurs**:
- Louis Fippo - louis.fippo@univ-yaounde1.cm

---

## ‚öñÔ∏è Licence

Distribu√© sous la licence MIT. Voir [LICENSE](LICENSE) pour plus d'informations.